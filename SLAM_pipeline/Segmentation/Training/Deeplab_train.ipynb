{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stebbibg/MSc_Fstudent_SLAM/blob/main/Deeplab_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EUymQ_nexFf",
        "outputId": "442b2994-7526-431b-e615-691808b50764"
      },
      "outputs": [],
      "source": [
        "\n",
        "#from google.colab import drive\n",
        "import PIL\n",
        "from PIL import Image\n",
        "#drive.mount('/content/drive')\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torch\n",
        "import torchvision.transforms.functional as TF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "h6CMnd7ZBn0P"
      },
      "outputs": [],
      "source": [
        "from skimage.util import random_noise\n",
        "import random\n",
        "#from google.colab import drive\n",
        "import PIL\n",
        "from PIL import Image\n",
        "\n",
        "# The validation images were not augmented\n",
        "def getValImg(img, mask):\n",
        "  return img, mask\n",
        "\n",
        "def getTrainImg(img, mask):\n",
        "  # The probabilities for which augmentations will be used\n",
        "  p_crop = 0.8\n",
        "  p_affine = 0.2\n",
        "  p_color_jitter = 00.5\n",
        "  p_sp = 0.1\n",
        "  p_speckle = 0.1\n",
        "  p_erase = 0.2\n",
        "\n",
        "  # Gaussian noise parameters\n",
        "  p_gauss = 0.5\n",
        "  kernel_size = 15\n",
        "  \n",
        "  if random.random() < p_gauss:\n",
        "    kernel_size = random.randrange(5, 25, 2)\n",
        "    color_jitter_t = torchvision.transforms.Compose([\n",
        "      torchvision.transforms.GaussianBlur(kernel_size, sigma=(0.1, 3.0)),\n",
        "    ])\n",
        "    img = color_jitter_t(img)\n",
        "\n",
        "  if random.random() < p_color_jitter:\n",
        "    color_jitter_t = torchvision.transforms.Compose([\n",
        "      torchvision.transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, hue=0.01)\n",
        "    ])\n",
        "    img = color_jitter_t(img)\n",
        "\n",
        "  # random affine\n",
        "  if random.random() < p_affine:\n",
        "    affine_params = torchvision.transforms.RandomAffine.get_params((-8, 8), (0.05, 0.05), (0.95, 0.95), (-8, 8), img.size)\n",
        "    img, mask = TF.affine(img, *affine_params), TF.affine(mask, *affine_params)\n",
        "\n",
        "  # Random crop\n",
        "  if random.random() < p_crop:\n",
        "    new_width = random.randint(1088, 1554)\n",
        "    new_height = random.randint(1456, 2080)\n",
        "    resize = torchvision.transforms.Resize(size=(new_width, new_height), interpolation=PIL.Image.NEAREST)\n",
        "    img = resize(img)\n",
        "    mask = resize(mask)\n",
        "\n",
        "    i, j, h, w = torchvision.transforms.RandomCrop.get_params(\n",
        "        img, output_size=(1088, 1456))\n",
        "    img = TF.crop(img, i, j, h, w)\n",
        "    mask = TF.crop(mask, i, j, h, w)\n",
        "  \n",
        "  # speckle noise\n",
        "  if random.random() < p_speckle:\n",
        "    img_sp = np.asarray(img)\n",
        "    img_sp = random_noise(img_sp, mode='speckle', mean=0.1, seed=42,\n",
        "                                var=0.2)    \n",
        "    img_sp = img_sp.transpose((2, 0, 1))\n",
        "    img = torch.from_numpy(img_sp)\n",
        "    \n",
        "    img = torchvision.transforms.ToPILImage()(img).convert(\"RGB\")\n",
        "\n",
        "  # salt and pepper noise\n",
        "  if random.random() < p_sp:\n",
        "    img_sp = np.asarray(img)\n",
        "    img_sp = random_noise(img_sp, mode='s&p', salt_vs_pepper=0.5, clip=True)\n",
        "    img_sp = img_sp.transpose((2, 0, 1))\n",
        "\n",
        "    img = torch.from_numpy(img_sp)\n",
        "    \n",
        "    img = torchvision.transforms.ToPILImage()(img).convert(\"RGB\")\n",
        "\n",
        "  if random.random() < p_erase:\n",
        "    mask_tensor = TF.to_tensor(mask)\n",
        "    img = TF.to_tensor(img)\n",
        "    i, j, h, w, v = torchvision.transforms.RandomErasing.get_params(img, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=[0])\n",
        "\n",
        "    # Make it to three channels\n",
        "    mask_tensor = mask_tensor.repeat(3,1,1)\n",
        "\n",
        "    img = TF.erase(img, i, j, h, w, v)\n",
        "    mask_tensor = TF.erase(mask_tensor, i, j, h, w, v)\n",
        "    # Extract the first channel\n",
        "    mask, _, _ = mask_tensor.unbind(0)\n",
        "    mask = TF.to_pil_image(mask)\n",
        "    img = TF.to_pil_image(img)\n",
        "\n",
        "  return img, mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1Ejkg6A5TItY"
      },
      "outputs": [],
      "source": [
        "from os.path import splitext\n",
        "from os import listdir\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import torchvision\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import logging\n",
        "from PIL import Image\n",
        "import torchvision.transforms.functional as TF\n",
        "import random\n",
        "\n",
        "#train_indx = []  # Indices for the training images\n",
        "#val_indx = []    # Indices for the validation images  \n",
        "\n",
        "class TrainingDataset(Dataset):\n",
        "    def __init__(self, imgs_dir, masks_dir, train_indx, val_indx, scale=0.5, mask_suffix=''):\n",
        "        self.train_indx = []\n",
        "        self.val_indx = []\n",
        "        self.imgs_dir = imgs_dir\n",
        "        self.masks_dir = masks_dir\n",
        "        self.scale = scale\n",
        "        self.mask_suffix = mask_suffix\n",
        "        assert 0 < scale <= 1, 'Scale must be between 0 and 1'\n",
        "\n",
        "        self.ids = [splitext(file)[0] for file in listdir(imgs_dir)\n",
        "                    if not file.startswith('.')]\n",
        "        logging.info(f'Creating dataset with {len(self.ids)} examples')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    @classmethod\n",
        "    def preprocess(cls, pil_img, scale):\n",
        "        w, h = pil_img.size\n",
        "        newW, newH = int(scale * w), int(scale * h)\n",
        "        assert newW > 0 and newH > 0, 'Scale is too small'\n",
        "        pil_img = pil_img.resize((newW, newH))\n",
        "\n",
        "        img_nd = np.array(pil_img)\n",
        "\n",
        "        if len(img_nd.shape) == 2:\n",
        "            img_nd = np.expand_dims(img_nd, axis=2)\n",
        "\n",
        "        # HWC to CHW\n",
        "        img_trans = img_nd.transpose((2, 0, 1))\n",
        "\n",
        "        return img_trans\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        idx = self.ids[i]\n",
        "        mask_file = glob(self.masks_dir + idx + self.mask_suffix + '.*')\n",
        "        img_file = glob(self.imgs_dir + idx + '.*')\n",
        "\n",
        "        mask = Image.open(mask_file[0])\n",
        "        img = Image.open(img_file[0])\n",
        "\n",
        "        if int(i) in self.train_indx:\n",
        "          #print(\"IMAGE IN TRAIN\")\n",
        "          img, mask = getTrainImg(img, mask)\n",
        "        else:\n",
        "          #print(\"IMAGE IN VAL\")\n",
        "          img, mask = getValImg(img, mask)\n",
        "        \n",
        "        img = self.preprocess(img, self.scale)\n",
        "        mask = self.preprocess(mask, self.scale)\n",
        "\n",
        "        img_tensor = torch.from_numpy(img).type(torch.FloatTensor)\n",
        "\n",
        "        preprocess_image = torchvision.transforms.Compose([\n",
        "            torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "        img_tensor = preprocess_image(img_tensor)\n",
        "\n",
        "        return {\n",
        "            'image': img_tensor,\n",
        "            'mask': torch.from_numpy(mask).type(torch.FloatTensor)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "L2NBUpj6AbNJ"
      },
      "outputs": [],
      "source": [
        "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
        "from torchvision.models.segmentation import deeplabv3_resnet101\n",
        "\n",
        "# A pretrained deeplab module\n",
        "def custom_DeepLabv3(out_channel):\n",
        "  model = deeplabv3_resnet101(pretrained=True)\n",
        "  # Make a new output layer\n",
        "  model.classifier = DeepLabHead(2048, out_channel)\n",
        "\n",
        "  #Set the model in training mode\n",
        "  model.train()\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "g7_fQyisPHm5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "60992090\n"
          ]
        }
      ],
      "source": [
        "# Counting the parameters in the module\n",
        "model = custom_DeepLabv3(5)\n",
        "ct = 0\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(count_parameters(model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "p69xjPI6QSfe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/2 [00:00<?, ?it/s]/tmp/ipykernel_14951/351261627.py:46: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
            "  resize = torchvision.transforms.Resize(size=(new_width, new_height), interpolation=PIL.Image.NEAREST)\n",
            "/home/agervig/.local/lib/python3.8/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  warnings.warn(\n",
            "/tmp/ipykernel_14951/351261627.py:46: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
            "  resize = torchvision.transforms.Resize(size=(new_width, new_height), interpolation=PIL.Image.NEAREST)\n",
            "/home/agervig/.local/lib/python3.8/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  warnings.warn(\n",
            "/tmp/ipykernel_14951/351261627.py:46: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
            "  resize = torchvision.transforms.Resize(size=(new_width, new_height), interpolation=PIL.Image.NEAREST)\n",
            "/home/agervig/.local/lib/python3.8/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  warnings.warn(\n",
            "/tmp/ipykernel_14951/351261627.py:46: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
            "  resize = torchvision.transforms.Resize(size=(new_width, new_height), interpolation=PIL.Image.NEAREST)\n",
            "/home/agervig/.local/lib/python3.8/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  warnings.warn(\n",
            "/tmp/ipykernel_14951/351261627.py:46: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
            "  resize = torchvision.transforms.Resize(size=(new_width, new_height), interpolation=PIL.Image.NEAREST)\n",
            "/home/agervig/.local/lib/python3.8/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  warnings.warn(\n",
            "/tmp/ipykernel_14951/351261627.py:46: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
            "  resize = torchvision.transforms.Resize(size=(new_width, new_height), interpolation=PIL.Image.NEAREST)\n",
            "/home/agervig/.local/lib/python3.8/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  warnings.warn(\n",
            "/tmp/ipykernel_14951/351261627.py:46: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
            "  resize = torchvision.transforms.Resize(size=(new_width, new_height), interpolation=PIL.Image.NEAREST)\n",
            "/home/agervig/.local/lib/python3.8/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  warnings.warn(\n",
            "/tmp/ipykernel_14951/351261627.py:46: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
            "  resize = torchvision.transforms.Resize(size=(new_width, new_height), interpolation=PIL.Image.NEAREST)\n",
            "/home/agervig/.local/lib/python3.8/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  warnings.warn(\n",
            "  0%|          | 0/2 [00:03<?, ?it/s]\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "CUDA out of memory. Tried to allocate 98.00 MiB (GPU 0; 7.92 GiB total capacity; 5.90 GiB already allocated; 112.94 MiB free; 6.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m/home/agervig/git/FSM/MSc_Fstudent_SLAM/Segmentation/Training/Deeplab_train.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 46>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/agervig/git/FSM/MSc_Fstudent_SLAM/Segmentation/Training/Deeplab_train.ipynb#X10sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m   loss \u001b[39m=\u001b[39m criterion(masks_pred[\u001b[39m'\u001b[39m\u001b[39mout\u001b[39m\u001b[39m'\u001b[39m], true_masks_flat)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/agervig/git/FSM/MSc_Fstudent_SLAM/Segmentation/Training/Deeplab_train.ipynb#X10sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m   current_epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/agervig/git/FSM/MSc_Fstudent_SLAM/Segmentation/Training/Deeplab_train.ipynb#X10sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m   loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/agervig/git/FSM/MSc_Fstudent_SLAM/Segmentation/Training/Deeplab_train.ipynb#X10sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m   optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/agervig/git/FSM/MSc_Fstudent_SLAM/Segmentation/Training/Deeplab_train.ipynb#X10sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 0; 7.92 GiB total capacity; 5.90 GiB already allocated; 112.94 MiB free; 6.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import os\n",
        "import sys\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "from sklearn.metrics import jaccard_score\n",
        "import tqdm\n",
        "\n",
        "device = torch.device('cuda:0')\n",
        "model.to(device)\n",
        "\n",
        "batch_size = 2\n",
        "img_scale = 0.5\n",
        "dir_img = '/home/agervig/git/FSM/MSc_Fstudent_SLAM/data/training_data/imgs/'\n",
        "dir_mask = '/home/agervig/git/FSM/MSc_Fstudent_SLAM/data/training_data/masks/'\n",
        "lr = 0.00002\n",
        "dir_checkpoint = '/home/agervig/git/FSM/MSc_Fstudent_SLAM/Segmentation/experiments/checkpoints/'\n",
        "dataset = TrainingDataset(dir_img, dir_mask,[], [], img_scale)\n",
        "\n",
        "n_val = 133\n",
        "n_test = 100\n",
        "n_train = 850\n",
        "train, val, test = random_split(dataset, [n_train, n_val, n_test])\n",
        "dataset.train_indx = train.indices\n",
        "dataset.val_indx = val.indices\n",
        "\n",
        "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
        "val_loader = DataLoader(val, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True, drop_last=True)\n",
        "test_loader = DataLoader(test, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True, drop_last=True)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "epochs = 2 #70\n",
        "# To load a specific state\n",
        "#model.load_state_dict(torch.load('/content/drive/My Drive/Colab Notebooks//checkpoints/8.pth'))\n",
        "train_loop = tqdm.tqdm(range(epochs))\n",
        "\n",
        "\n",
        "\n",
        "for i in (train_loop):\n",
        "  current_epoch_loss = 0\n",
        "  model.train()\n",
        "  ctr = 0\n",
        "  for batch in train_loader:\n",
        "    ctr += 1\n",
        "    optimizer.zero_grad()\n",
        "    imgs = batch['image']\n",
        "    true_masks = batch['mask']\n",
        "    imgs = imgs.to(device=device, dtype=torch.float32)\n",
        "    mask_type = torch.float32\n",
        "    true_masks = true_masks.to(device=device, dtype=torch.long)\n",
        "    masks_pred = model(imgs)\n",
        "\n",
        "    true_masks_flat = torch.squeeze(true_masks, dim=0)\n",
        "    true_masks_flat = true_masks_flat.squeeze(1)\n",
        "\n",
        "    loss = criterion(masks_pred['out'], true_masks_flat)\n",
        "\n",
        "    current_epoch_loss += loss.item()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  model.eval()\n",
        "  val_loss = 0\n",
        "  for batch in val_loader:\n",
        "    imgs = batch['image']\n",
        "    true_masks = batch['mask']\n",
        "    imgs = imgs.to(device=device, dtype=torch.float32)\n",
        "    mask_type = torch.float32\n",
        "    true_masks = true_masks.to(device=device, dtype=torch.long)\n",
        "    masks_pred = model(imgs)\n",
        "\n",
        "    true_masks_flat = torch.squeeze(true_masks, dim=1)\n",
        "    true_masks_flat = true_masks_flat.squeeze(1)\n",
        "\n",
        "    loss = criterion(masks_pred['out'], true_masks_flat)\n",
        "    val_loss += loss.item()\n",
        "  total_iou = 0\n",
        "  model.eval()\n",
        "  test_counter = 0\n",
        "  for batch in test_loader:\n",
        "    imgs = batch['image']\n",
        "    true_masks = batch['mask']\n",
        "    imgs = imgs.to(device=device, dtype=torch.float32)\n",
        "    mask_type = torch.float32\n",
        "    true_masks = true_masks.to(device=device, dtype=torch.long)\n",
        "    masks_pred = model(imgs)['out']\n",
        "\n",
        "    masks_pred = torch.argmax(masks_pred, axis=1)\n",
        "\n",
        "    true_masks_flat = torch.squeeze(true_masks, dim=1)\n",
        "\n",
        "    true_masks_flat = true_masks_flat.cpu().numpy().reshape(-1)\n",
        "    masks_pred = masks_pred.cpu().numpy().reshape(-1)\n",
        "\n",
        "    iou = jaccard_score(true_masks_flat, masks_pred, labels=[1, 2, 3, 4], average= 'micro')\n",
        "    total_iou += iou\n",
        "    test_counter += 1\n",
        "  \n",
        "  iou = total_iou /test_counter\n",
        "\n",
        "  current_epoch_loss /= n_train\n",
        "  val_loss /= n_val\n",
        "  print(\"epoch: \" + str(i + 1) + \" training loss: \" + str(current_epoch_loss) + \" val loss: \" + str(val_loss) + \" iou: \" + str(iou) + \"\\n\")\n",
        "  file1 = open(\"/home/agervig/git/FSM/MSc_Fstudent_SLAM/Segmentation/experiments/training.txt\", \"a\")  #\"/content/drive/My Drive/Colab Notebooks/training.txt\"\n",
        "  file1.write(str(current_epoch_loss) + \" \" + str(val_loss) + \" \" + str(iou) + \"\\n\")\n",
        "  torch.save(copy.deepcopy(model.state_dict()), dir_checkpoint + str(i) + \".pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mt1sNORhu-y-"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6yB9oAGW-tl"
      },
      "source": [
        "# New Section\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyM8o8IskTtL1u3L0ZuK97D4",
      "collapsed_sections": [],
      "include_colab_link": true,
      "machine_shape": "hm",
      "name": "Deeplab_train",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
